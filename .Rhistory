#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Sample the list of files, if sample_document is higher than 0 and smaller than the number of files.
if (sample_document > 0 & length(text_files) > sample_document) {
text_files = sample(text_files, sample_document)
}
#Retrieve all the texts and initialize the classification dataset.
if (metadata_only == FALSE) {
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readChar, nchar = max_char))
} else {
text_data = data_frame(document = basename(text_files),
label = "")
}
if (output_format == "tsv") {
write_tsv(text_data, training_file)
}
if (output_format == "csv") {
write_csv(text_data, training_file)
}
}
tds_generate(dir = "test_corpora")
metadata_only = FALSE
training_file = ""
dir = "test_corpora"
output_format = "tsv"
max_char = 50000
max_char = 500000
#output_format has to be either csv or tsv.
if (!output_format %in% c("tsv", "csv")) {
output_format = "tsv"
}
#If a training file has not been stated, we will reuse the name of the directory.
if (training_file == "") {
training_file = paste0(dir, ".", output_format)
}
#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Sample the list of files, if sample_document is higher than 0 and smaller than the number of files.
if (sample_document > 0 & length(text_files) > sample_document) {
text_files = sample(text_files, sample_document)
}
sample_document = 0
#Sample the list of files, if sample_document is higher than 0 and smaller than the number of files.
if (sample_document > 0 & length(text_files) > sample_document) {
text_files = sample(text_files, sample_document)
}
#Retrieve all the texts and initialize the classification dataset.
if (metadata_only == FALSE) {
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readChar, nchar = max_char))
} else {
text_data = data_frame(document = basename(text_files),
label = "")
}
text_data
#' A function to generate a csv file for training out of a directory of text
#'
#' @param dir A directory containing the training corpora, with one document per text file.
#' @param training_file The name of the file containing the output a the function: a dataset linking document name (and, possibly, texts) with empty labels.
#' @param metadata_only Whether the output will contain only the metadata (the name of the documents) or also the text. Set to false by default.
#' @param sample_document A sample size smaller than the number of the document in the directory. When set at 0 (default), sampling is not used.
#' @param output_format The format of the file where the data frame will be stored. Either "tsv" or "csv": "tsv" is usually preferable for textual data, since the tab key is much less used than punctuation.
#' @return A data frame prepared for classification labelling.
#' @examples
#'  tds_generate(dir = "corpora", training_file = "my_corpora.csv", metadata_only = TRUE, sample_document = 500, output_format = "csv")
tds_generate <- function(dir, training_file = "", metadata_only = FALSE, sample_document = 0, output_format = "tsv", max_char = 500000) {
#output_format has to be either csv or tsv.
if (!output_format %in% c("tsv", "csv")) {
output_format = "tsv"
}
#If a training file has not been stated, we will reuse the name of the directory.
if (training_file == "") {
training_file = paste0(dir, ".", output_format)
}
#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Sample the list of files, if sample_document is higher than 0 and smaller than the number of files.
if (sample_document > 0 & length(text_files) > sample_document) {
text_files = sample(text_files, sample_document)
}
#Retrieve all the texts and initialize the classification dataset.
if (metadata_only == FALSE) {
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readChar, nchar = max_char))
text_data = text_data %>% unnest(text)
} else {
text_data = data_frame(document = basename(text_files),
label = "")
}
if (output_format == "tsv") {
write_tsv(text_data, training_file)
}
if (output_format == "csv") {
write_csv(text_data, training_file)
}
}
tds_generate(dir = "test_corpora")
test = read_tsv("test_corpora.tsv")
test
tds_generate(dir = "test_corpora", metadata_only = TRUE)
tds_generate(dir = "test_corpora", sample_document = 10)
test = read_tsv("test_corpora.tsv")
test
tds_generate(dir = "test_corpora", metadata_only = TRUE)
training_corpus = tds_associate(dir = "test_corpora", training_metadata = "test_corpora.csv")
#' A function to combine the text with a labelled set of metadata.
#'
#' @param dir A directory containing the training corpora, with one document per text file.
#' @param training_metadata The name of the file containing the name of the name of the documents, the labels and possibly other metadata.
#' @param csv_type A type of csv file: "comma" (by default) uses read_csv while "semicolon" uses read_csv2.
#' @return A data frame combining the texts located in a directory and the labelled data in a tsv/csv file.
#' @examples
#'  tds_associate(dir = "corpora", training_metadata = "my_corpora.csv")
tds_associate <- function(dir, training_metadata = "", csv_type = "comma") {
#Open the training_metadata file with either tsv or csv
if (grepl("tsv$", training_metadata)) {
training_metadata = read_tsv(training_metadata)
}
if (grepl("csv$", training_metadata)) {
if(csv_type == "comma") {
training_metadata = read_csv(training_metadata)
}
if(csv_type == "semicolon"){
training_metadata = read_csv2(training_metadata)
}
}
#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Create the text data frame.
text_data = data_frame(document = basename(text_files),
text = map(text_files, readChar, nchar = max_char))
#Join the metadata on the text data frame.
text_data = text_data %>%
inner_join(training_metadata, by=c("document" = "document"))
return(text_data)
}
training_corpus = tds_associate(dir = "test_corpora", training_metadata = "test_corpora.csv", csv_type = "semicolon")
training_corpus
#' A function to combine the text with a labelled set of metadata.
#'
#' @param dir A directory containing the training corpora, with one document per text file.
#' @param training_metadata The name of the file containing the name of the name of the documents, the labels and possibly other metadata.
#' @param csv_type A type of csv file: "comma" (by default) uses read_csv while "semicolon" uses read_csv2.
#' @return A data frame combining the texts located in a directory and the labelled data in a tsv/csv file.
#' @examples
#'  tds_associate(dir = "corpora", training_metadata = "my_corpora.csv")
tds_associate <- function(dir, training_metadata = "", csv_type = "comma") {
#Open the training_metadata file with either tsv or csv
if (grepl("tsv$", training_metadata)) {
training_metadata = read_tsv(training_metadata)
}
if (grepl("csv$", training_metadata)) {
if(csv_type == "comma") {
training_metadata = read_csv(training_metadata)
}
if(csv_type == "semicolon"){
training_metadata = read_csv2(training_metadata)
}
}
#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Create the text data frame.
text_data = data_frame(document = basename(text_files),
text = map(text_files, readChar, nchar = max_char))
text_data = text_data %>% unnest(text)
#Join the metadata on the text data frame.
text_data = text_data %>%
inner_join(training_metadata, by=c("document" = "document"))
return(text_data)
}
training_corpus = tds_associate(dir = "test_corpora", training_metadata = "test_corpora.csv", csv_type = "semicolon")
training_corpus
library(tidysupervise)
??unnest
library(roxygen2)
roxygenise()
library(tidysupervise)
library(roxygen2)
roxygenise()
library(tidysupervise)
library(tidysupervise)
?importFrom
??importFrom
library(tidysupervise)
library(roxygen2)
roxygenise()
library(tidysupervise)
install_github("Numapresse/TidySupervise")
devtools::install_github("Numapresse/TidySupervise")
library(tidysupervise)
?tds_apply
library(tidysupervise)
complete_text_classification = readr::read_tsv("corpus_classification.tsv.zip")
complete_text_classification = readr::read_tsv("../tidy_supervise/corpus_classification.tsv.zip")
training_corpus = tds_associate(dir = "../tidy_supervise/test_corpora", training_metadata = "../tidy_supervise/test_corpora.csv", csv_type = "semicolon")
training_corpus
text_count = training_corpus %>% tidytext::unnest_tokens(token, text)
test_model_2 = text_count %>%
tds_process(lemmatization = "french", segment_size = 90, min_doc_count = 2, max_word_set = 3000)
#' @param text_count A tidy dataframe of textual data (typically the "long" format used by tidytext)
#' @param segment_size The standard size of a continuous segment of text. Remaining segments within a document will be discarded. If set to 0 (default), documents will be used as the main units.
#' @param no_number Set to true to remove all numbers from the dataset. This is usually recommended.
#' @param lemmatization Use one of the five available languages for lemmatization: "english", "french", "german", "spanish" or "italian". Lemmatization is approximative but usually enhance the results (since inflected words can be more readily used)
#' @param min_doc_count The minimum number of documents where a word should be present. It's useful to avoid over-fitting.
#' @param min_word_count The minimum number of occurrences of a word all documents considered.
#' @param max_word_set The maximum number of word to keep (3000 as a default). Massive matrix becomes harder to process for little gain.
#' @return A processed version of the textual dataset using tf_idf values with cosine normalization.
#' @examples
#'  tds_clean(text_count, segment_size = 90, min_doc_count = 4, max_word_set = 3000)
tds_process <- function(text_count, segment_size = 0, training = FALSE, lemmatization = "none", no_number = TRUE, min_doc_count = 2, min_word_count = 0, max_word_set = 3000) {
#Cleaning the numbers (recommended)
if (no_number == TRUE) {
text_count <- text_count %>% filter(!grepl("\\d+", token))
}
#Using lemmatization (recommended but not always available and need to now the language in advance: default to none)
available_lemmatization = c("french", "english", "spanish", "german", "italian")
if (lemmatization %in% available_lemmatization) {
#We load the zipped lookup files when they exist.
lemmatization_file = paste0("lookup_", lemmatization, ".tsv.zip")
lookup = readr::read_tsv(system.file("extdata", lemmatization_file, package = "tidysupervise"), col_types = "cc")
#We transform token to lemma. Every token that is not in the lookup table is kept as such.
text_count = text_count %>%
left_join(lookup, by=c("token" = "token")) %>%
mutate(lemma = ifelse(is.na(lemma), token, lemma)) %>%
select(-token) %>%
rename(token = lemma)
}
#If segments are > to 0, we initialize the segments and filter every segments that is lower than the mean segment size.
#Otherwise, we just group everything on the documents.
text_count <- text_count %>%
group_by(document) %>%
mutate(word_id = 1:n()) %>%
ungroup()
if (segment_size > 0) {
text_count <- text_count %>%
mutate(segment = paste0(document, "_", (word_id+segment_size)%/%segment_size)) %>%
ungroup() %>%
group_by(segment) %>%
mutate(total_segment = max(1:n())+1) %>%
filter(total_segment>=segment_size)
text_count <- text_count %>% group_by(document, label, segment, token) %>% summarise(count = n()) %>% ungroup()
} else {
text_count <- text_count %>% group_by(document, segment, token) %>% summarise(count = n()) %>% ungroup()
}
#We filter all the occurrences that do not appear in n different documents (n = min_doc_count)
#Default is 2 (as word occurring in only one document will not be that useful)
#We also filter the words that are below the nth row in descending occurrences order.
#Default is 3000. Otherwhise, the matrix can become very heavy for little gains.
#Both filters are done in one batch to avoid code duplication.
if (min_doc_count > 0 | max_word_set > 0) {
occur_doc <- text_count %>% group_by(token, document) %>% summarise() %>% ungroup() %>% group_by(token) %>% summarise(doc_count = n())
if(min_doc_count > 0) {
occur_doc = occur_doc %>% filter(doc_count>=min_doc_count)
}
#We ensure that the word set is smaller than max_word_set
if(max_word_set > 0 & nrow(occur_doc) > max_word_set) {
occur_doc = occur_doc %>% arrange(-doc_count) %>% mutate(rank = min_rank(desc(doc_count))) %>% filter(rank < max_word_set)
occur_doc = occur_doc %>% filter(rank < max(rank))
}
#We keep only the word that are still selected
text_count <- text_count %>% inner_join(occur_doc %>% select(token), by=c("token" = "token"))
}
#We filter all the occurrences that do not appear at least n time.
if (min_word_count > 0) {
text_count <- text_count %>% group_by(token) %>% mutate(word_count = n()) %>% filter(word_count >= min_word_count) %>% ungroup()
}
#We extract the tf_idf values thanks to the tidy text package
if(segment_size > 0) {
#We extract the tf_idf values thanks to the tidy text package
text_count <- text_count %>% bind_tf_idf(token, segment, count)
#We apply cosine normalization (essential to get good svm results)
text_count <- text_count %>% group_by(segment) %>% mutate(cosine_variable = sqrt(sum(tf_idf^2)))
text_count <- text_count %>% ungroup() %>% mutate(corrected_tf_idf = tf_idf/cosine_variable)
} else {
#Same as before, only this time we group on the documents (since the segments do not exist)
#We extract the tf_idf values thanks to the tidy text package
text_count <- text_count %>% bind_tf_idf(token, document, count)
#We apply cosine normalization (essential to get good svm results)
text_count <- text_count %>% group_by(document) %>% mutate(cosine_variable = sqrt(sum(tf_idf^2)))
text_count <- text_count %>% ungroup() %>% mutate(corrected_tf_idf = tf_idf/cosine_variable)
}
return(text_count)
}
test_model_2 = text_count %>%
tds_process(lemmatization = "french", segment_size = 90, min_doc_count = 2, max_word_set = 3000)
library(tidysupervise)
#' @param text_count A tidy dataframe of textual data (typically the "long" format used by tidytext)
#' @param segment_size The standard size of a continuous segment of text. Remaining segments within a document will be discarded. If set to 0 (default), documents will be used as the main units.
#' @param no_number Set to true to remove all numbers from the dataset. This is usually recommended.
#' @param lemmatization Use one of the five available languages for lemmatization: "english", "french", "german", "spanish" or "italian". Lemmatization is approximative but usually enhance the results (since inflected words can be more readily used)
#' @param min_doc_count The minimum number of documents where a word should be present. It's useful to avoid over-fitting.
#' @param min_word_count The minimum number of occurrences of a word all documents considered.
#' @param max_word_set The maximum number of word to keep (3000 as a default). Massive matrix becomes harder to process for little gain.
#' @return A processed version of the textual dataset using tf_idf values with cosine normalization.
#' @examples
#'  tds_clean(text_count, segment_size = 90, min_doc_count = 4, max_word_set = 3000)
tds_process <- function(text_count, segment_size = 0, training = FALSE, lemmatization = "none", no_number = TRUE, min_doc_count = 2, min_word_count = 0, max_word_set = 3000) {
#Cleaning the numbers (recommended)
if (no_number == TRUE) {
text_count <- text_count %>% filter(!grepl("\\d+", token))
}
#Using lemmatization (recommended but not always available and need to now the language in advance: default to none)
available_lemmatization = c("french", "english", "spanish", "german", "italian")
if (lemmatization %in% available_lemmatization) {
#We load the zipped lookup files when they exist.
lemmatization_file = paste0("lookup_", lemmatization, ".tsv.zip")
lookup = readr::read_tsv(system.file("extdata", lemmatization_file, package = "tidysupervise"), col_types = "cc")
#We transform token to lemma. Every token that is not in the lookup table is kept as such.
text_count = text_count %>%
left_join(lookup, by=c("token" = "token")) %>%
mutate(lemma = ifelse(is.na(lemma), token, lemma)) %>%
select(-token) %>%
rename(token = lemma)
}
#If segments are > to 0, we initialize the segments and filter every segments that is lower than the mean segment size.
#Otherwise, we just group everything on the documents.
text_count <- text_count %>%
group_by(document) %>%
mutate(word_id = 1:n()) %>%
ungroup()
if (segment_size > 0) {
text_count <- text_count %>%
mutate(segment = paste0(document, "_", (word_id+segment_size)%/%segment_size)) %>%
ungroup() %>%
group_by(segment) %>%
mutate(total_segment = max(1:n())+1) %>%
filter(total_segment>=segment_size)
text_count <- text_count %>% group_by(document, label, segment, token) %>% summarise(count = n()) %>% ungroup()
} else {
text_count <- text_count %>% group_by(document, segment, token) %>% summarise(count = n()) %>% ungroup()
}
#We filter all the occurrences that do not appear in n different documents (n = min_doc_count)
#Default is 2 (as word occurring in only one document will not be that useful)
#We also filter the words that are below the nth row in descending occurrences order.
#Default is 3000. Otherwhise, the matrix can become very heavy for little gains.
#Both filters are done in one batch to avoid code duplication.
if (min_doc_count > 0 | max_word_set > 0) {
occur_doc <- text_count %>% group_by(token, document) %>% summarise() %>% ungroup() %>% group_by(token) %>% summarise(doc_count = n())
if(min_doc_count > 0) {
occur_doc = occur_doc %>% filter(doc_count>=min_doc_count)
}
#We ensure that the word set is smaller than max_word_set
if(max_word_set > 0 & nrow(occur_doc) > max_word_set) {
occur_doc = occur_doc %>% arrange(-doc_count) %>% mutate(rank = min_rank(desc(doc_count))) %>% filter(rank < max_word_set)
occur_doc = occur_doc %>% filter(rank < max(rank))
}
#We keep only the word that are still selected
text_count <- text_count %>% inner_join(occur_doc %>% select(token), by=c("token" = "token"))
}
#We filter all the occurrences that do not appear at least n time.
if (min_word_count > 0) {
text_count <- text_count %>% group_by(token) %>% mutate(word_count = n()) %>% filter(word_count >= min_word_count) %>% ungroup()
}
#We extract the tf_idf values thanks to the tidy text package
if(segment_size > 0) {
#We extract the tf_idf values thanks to the tidy text package
text_count <- text_count %>% bind_tf_idf(token, segment, count)
#We apply cosine normalization (essential to get good svm results)
text_count <- text_count %>% group_by(segment) %>% mutate(cosine_variable = sqrt(sum(tf_idf^2)))
text_count <- text_count %>% ungroup() %>% mutate(corrected_tf_idf = tf_idf/cosine_variable)
} else {
#Same as before, only this time we group on the documents (since the segments do not exist)
#We extract the tf_idf values thanks to the tidy text package
text_count <- text_count %>% bind_tf_idf(token, document, count)
#We apply cosine normalization (essential to get good svm results)
text_count <- text_count %>% group_by(document) %>% mutate(cosine_variable = sqrt(sum(tf_idf^2)))
text_count <- text_count %>% ungroup() %>% mutate(corrected_tf_idf = tf_idf/cosine_variable)
}
return(text_count)
}
library(tidysupervise)
training_corpus = tds_associate(dir = "../tidy_supervise/test_corpora", training_metadata = "../tidy_supervise/test_corpora.csv", csv_type = "semicolon")
text_count = training_corpus %>% tidytext::unnest_tokens(token, text)
test_model_2 = text_count %>%
tds_process(lemmatization = "french", segment_size = 90, min_doc_count = 2, max_word_set = 3000)
test_model_2
test_model_2 = text_count %>%
tds_process(lemmatization = "french", segment_size = 0, min_doc_count = 2, max_word_set = 3000)
library(tidysupervise)
training_corpus = tds_associate(dir = "../tidy_supervise/test_corpora", training_metadata = "../tidy_supervise/test_corpora.csv", csv_type = "semicolon")
text_count = training_corpus %>% tidytext::unnest_tokens(token, text)
test_model_2 = text_count %>%
tds_process(lemmatization = "french", segment_size = 0, min_doc_count = 2, max_word_set = 3000)
test_model_2
test_model_2 = text_count %>%
tds_process(lemmatization = "french", training = TRUE, segment_size = 0, min_doc_count = 2, max_word_set = 3000)
test_model_2
test_model_2 = text_count %>%
tds_process(lemmatization = "french", training = FALSE, segment_size = 90, min_doc_count = 2, max_word_set = 3000)
test_model_2
#'
#' @param text_count A tidy dataframe of textual data pre-processed by tds_process
#' @param classification_matrix A term/document matrix. Set to false (by default) to create the matrix from text_count.
#' @param prob_state Set to true (default) to get the detailed probabilities. This is highly recommended but can take longer.
#' @param min_label The minimal number of document per labels. If some labels are poorly represented and harder to predict it can be useful to drop them.
#' @param max_label The maximal number of document per labels. SVM is sensible to over-fitting: if a label is too represented in the original corpora, it will over-populate the results.
#' @param cost_variable A parametrized value for svm classification (purely optional)
#' @return An SVM model that can be interpreted using tds_open.
#' @examples
#'  tds_model(text_count, classification_matrix = FALSE, prob_state = TRUE, min_label = 70, max_label = 150, cost_variable = 0)
tds_model <- function(text_count, classification_matrix = FALSE, prob_state = TRUE, min_label = 20, max_label = 100, cost_variable = 0){
#A small hack to take into account the use of entire documents for classification purposes.
if(!"segment" %in% colnames(text_count)) {
text_count = text_count %>% rename(segment = document)
}
if (classification_matrix == FALSE) {
classification_matrix = tds_matrix(text_count)
}
#We retrieve the name and label of each document.
classification_code <- data_frame(segment = row.names(classification_matrix))
classification_code <- classification_code %>%
left_join(text_count %>% group_by(segment, label) %>% summarise(), by=c("segment" = "segment")) %>%
mutate(labels = as.factor(label))
if(min_label != 0 | max_label != 0) {
#We apply several filters to the document selection (mostly by creating a random selection per label)
classification_code <- classification_code %>% group_by(label) %>% mutate(total_label=n()) %>% ungroup()
#A default in case max_label would be ill-defined (for instance bigger than min_label): it is switched off to 0.
if(max_label <= min_label & max_label > 0) {
max_label = 0
message("max_label couldn't be used as it shouldn't be smaller than min_label")
}
if(min_label > 0) {
classification_code <- classification_code %>% filter(total_label>min_label) %>% ungroup()
}
if(max_label > 0) {
sample_label <- classification_code %>% filter(total_label>max_label) %>% group_by(label) %>% sample_n(max_label) %>% ungroup()
classification_code <- classification_code %>% filter(total_label<max_label) %>% bind_rows(sample_label)
classification_code <- classification_code %>% sample_n(nrow(classification_code))
}
#We filter the original matrix.
classification_matrix <- classification_matrix[classification_code$segment,]
}
#Security check if some column sums are not equal to zero.
i <- (colSums(classification_matrix, na.rm=T) != 0)
classification_matrix <- classification_matrix[,i]
#Actual modeling.
if (cost_variable == 0) {
fit <- svm(classification_matrix, classification_code$labels, kernel = "linear", probability = prob_state)
} else {
fit <- svm(classification_matrix, classification_code$labels, kernel = "linear", probability = prob_state, cost = cost_variable)
}
return(fit)
}
test_model = tds_model(test_model_2, min_label = 0, max_label = 25)
test_model_2 = text_count %>%
tds_process(lemmatization = "french", training = TRUE, segment_size = 90, min_doc_count = 2, max_word_set = 3000)
test_model = tds_model(test_model_2, min_label = 0, max_label = 25)
library(e1071)
test_model = tds_model(test_model_2, min_label = 0, max_label = 25)
test_model
tds_open(test_model)
text_apply = readr::read_tsv("inst/extdata/apply_classification.tsv") %>% filter(grepl("1924-08-11", date))
text_apply = readr::read_tsv("inst/extdata/apply_classification.tsv.zip") %>% filter(grepl("1924-08-11", date))
text_apply = readr::read_tsv("inst/extdata/apply_classification.tsv.zip") %>% filter(grepl("1924-08-11", document))
text_apply
text_apply = readr::read_tsv("inst/extdata/apply_classification.tsv.zip")
text_apply
text_apply = text_apply %>%
tidytext::unnest_tokens(token, text) %>%
tds_process(segment_size = 0, min_doc_count = 2, max_word_set = 0, lemmatization = "french") %>%
tds_apply(model = test_model)
#' A function to apply an SVM model to a new set of text data
#'
#' @param text_count A new tidy dataframe of textual data pre-processed by tds_process
#' @param model A model generated by tds_model.
#' @param classification_matrix A term/document matrix. Set to false (by default) to create the matrix from text_count.
#' @param prob_state Set to true (default) to get the detailed probabilities. This is highly recommended but can take longer.
#' @param tidy_results Set to true (default) to get the results in a tidy format.
#' @return The results of the SVM model.
tds_apply <- function(text_count, model, classification_matrix = FALSE, prob_state = TRUE, tidy_results = TRUE) {
#A small hack to take into account the use of entire documents for classification purposes.
if(!"segment" %in% colnames(text_count)) {
text_count = text_count %>% rename(segment = document)
}
#Just in case, the user would prefer to pass their own matrix.
if (classification_matrix == FALSE) {
classification_matrix = tds_matrix(text_count)
}
#We ensure that the new word matrix only contains words listed in the model.
model_words <- dimnames(model$SV)[[2]] #The words of the model are nested inside $SV
no_model_words <- model_words[!(model_words %in% colnames(classification_matrix))] #We create a list of words that are not in the new matrix.
match_model_words <- model_words[model_words %in% colnames(classification_matrix)] #We create a list of words that are shared by the new matrix and the model.
new_matrix_corrected <- as.data.frame(classification_matrix[,match_model_words])
#In case there is words in the models that are not in the new matrix, we state them at 0.
new_matrix_corrected[,no_model_words] <- 0
new_matrix_corrected <- as.matrix(new_matrix_corrected)
new_matrix_corrected <- new_matrix_corrected[,model_words]
#Everything is ready for actual predictions.
predictions <- predict(model, new_matrix_corrected, probability=prob_state)
if(prob_state == FALSE) {
predictions <- data_frame(document = names(predictions), label_predicted = predictions)
}
if(prob_state == TRUE & tidy_results == TRUE) {
predictions <- tds_retrieve(predictions)
}
return(predictions)
}
text_apply = text_apply %>%
tidytext::unnest_tokens(token, text) %>%
tds_process(segment_size = 0, min_doc_count = 2, max_word_set = 0, lemmatization = "french") %>%
tds_apply(model = test_model)
text_apply
library(tidysupervise)
library(readr)
library(tidytext)
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus
