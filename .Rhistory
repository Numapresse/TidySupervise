tds_process(segment_size = 90, min_doc_count = 2, max_word_set = 0, lemmatization = "french") %>%
tds_apply(model = test_model)
text_apply = text_apply %>%
tidytext::unnest_tokens(token, text) %>%
tds_process(segment_size = 90, min_doc_count = 2, max_word_set = 0, lemmatization = "french") %>%
tds_apply(model = test_model)
text_count = text_apply %>% tidytext::unnest_tokens(token, text)
no_number == TRUE
no_number = TRUE
#Cleaning the numbers (recommended)
if (no_number == TRUE) {
text_count <- text_count %>% filter(!grepl("\\d+", token))
}
text_count
text_apply = readr::read_tsv("corpus_classification.tsv") %>% filter(grepl("1924_08_11", document))
text_apply
text_apply = readr::read_tsv("corpus_classification.tsv") %>% filter(grepl("1924-08-11", date))
text_apply
text_apply %>% group_by(page) %>% summarise()
text_apply = text_apply %>%
tidytext::unnest_tokens(token, text) %>%
tds_process(segment_size = 90, min_doc_count = 2, max_word_set = 0, lemmatization = "french") %>%
tds_apply(model = test_model)
text_apply
devtools::use_vignette("introduction")
library(tidysupervise)
?tds_matrix
??data_frame
dir = "../tidy_supervise/test_corpora"
library(tidyverse)
text_files = list.files(dir = dir, full.names = TRUE)
text_files = list.files(dir, full.names = TRUE)
text_files
basename(text_files)
text_data = data_frame(document = basename(text_files),
label = "",
text = map_dfr(text_files, readLines, warn = FALSE))
map_dfr(text_files, readLines, warn = FALSE)
map_dfr(text_files, readLines)
warnings()
?readLines
text_data = data_frame(document = basename(text_files),
label = "",
text = map_dfr(text_files, scan, what = "character"))
text_data = data_frame(document = basename(text_files),
label = "",
text = map_dfr(text_files, read_tsv))
text_data = data_frame(document = basename(text_files),
label = "",
text = map_dfr(text_files, read_lines))
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readLines))
text_data
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readLines, warning = FALSE))
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readLines, warnings = FALSE))
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readLines, warn = FALSE))
text_data = text_data %>% unnest(text)
text_data
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readLines, warn = FALSE))
text_data
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, scan, what = "character"))
text_data
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readChar, nchar = 500000))
text_data
text_data = text_data %>% unnest(text)
text_data
dir = "../tidy_supervise/test_corpora"
text_files = list.files(dir, full.names = TRUE)
random_document = 10
text_files = sample(text_files, random_document)
text_files
roxygenise()
library(roxygen2)
roxygenise()
text_classification = readr::read_tsv("corpus_classification.tsv")
text_classification %>% group_by(date) %>% summarise() %>% sample_n(6)
library(tidyverse)
text_classification %>% group_by(date) %>% summarise() %>% sample_n(6)
more_selection = text_classification %>% group_by(date) %>% summarise() %>% sample_n(6) %>% inner_join(text_classification)
more_selection
text_classification = more_selection
complete_text_classification = readr::read_tsv("corpus_classification.tsv")
text_count = text_classification %>% tidytext::unnest_tokens(token, text)
text_count
test_model_2 = text_count %>%
tds_process(training = TRUE, lemmatization = "french", segment_size = 90, min_doc_count = 2, max_word_set = 3000)
library(tidysupervise)
library(tidysupervise)
test_model_2 = text_count %>%
tds_process(training = TRUE, lemmatization = "french", segment_size = 90, min_doc_count = 2, max_word_set = 3000)
library(tidyverse)
test_model_2 = text_count %>%
tds_process(training = TRUE, lemmatization = "french", segment_size = 90, min_doc_count = 2, max_word_set = 3000)
library(dplyr)
library(tidyverse)
library(tidysupervise)
complete_text_classification = readr::read_tsv("corpus_classification.tsv")
text_classification = complete_text_classification %>% group_by(date) %>% summarise() %>% sample_n(6) %>% inner_join(complete_text_classification)
library(magrittr)
text_classification = complete_text_classification %>% group_by(date) %>% summarise() %>% sample_n(6) %>% inner_join(complete_text_classification)
library(dplyr)
library(tidysupervise)
complete_text_classification = readr::read_tsv("corpus_classification.tsv")
text_classification = complete_text_classification %>% group_by(date) %>% summarise() %>% sample_n(6) %>% inner_join(complete_text_classification)
library(tidysupervise)
text_classification = complete_text_classification %>% group_by(date) %>% summarise() %>% sample_n(6) %>% inner_join(complete_text_classification)
write_csv(text_classification, "sample_classification.csv")
library(readr)
write_csv(text_classification, "sample_classification.csv")
library(tidysupervise)
training_corpus = read_tsv(system.file("extdata", "corpus_classification.tsv.zip", package = "tidysupervise"))
training_corpus = read_tsv(system.file("extdata", "corpus_classification.tsv.zip", package = "tidysupervise"), col_types = "ccccccc")
training_corpus = unnest_token(training_corpus, token, text)
library(tidytext)
training_corpus = unnest_token(training_corpus, token, text)
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus
training_corpus %>% select(document, label, text)
training_corpus = read_tsv(system.file("extdata", "corpus_classification.tsv.zip", package = "tidysupervise"), col_types = "ccccccc")
training_corpus
complete_text_classification = readr::read_tsv("corpus_classification.tsv")
complete_text_classification = readr::read_tsv("corpus_classification.tsv.zip")
complete_text_classification %>% group_by(document)
complete_text_classification %>% sample_n(1500)
complete_text_classification %>% sample_n(1500) %>% arrange(date, document)
complete_text_classification %>% sample_n(1500) %>% arrange(date, document) %>% select(-work, -date, -page, -block)
complete_text_classification %>% sample_n(1500) %>% arrange(date, document) %>% select(-work, -date, -page, -block) %>% write_csv("sample_classification.csv")
complete_text_classification %>% sample_n(1500) %>% arrange(date, document) %>% select(-work, -date, -page, -block) %>% write_tsv("sample_classification.tsv")
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
library(tidysupervise)
library(readr)
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus
complete_text_classification %>% group_by(date) %>% summarise()
select_date = complete_text_classification %>% group_by(date) %>% summarise() %>% sample_n()
select_date = complete_text_classification %>% group_by(date) %>% summarise() %>% sample_n(20)
select_date
complete_text_classification %>% filter(date %in% select_date$date[1:15]) sample_n(1500) %>% arrange(date, document) %>% select(-work, -date, -page, -block) %>% write_tsv("sample_classification.tsv")
complete_text_classification %>% filter(date %in% select_date$date[1:15]) %>% sample_n(1500) %>% arrange(date, document) %>% select(-work, -date, -page, -block) %>% write_tsv("sample_classification.tsv")
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus
training_corpus_processed = tds_process(training_corpus)
training_corpus_processed = tds_process(training_corpus)
training_corpus_processed
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus %>%
unnest_token(token, text) %>%
tds_process()
training_corpus %>%
unnest_tokens(token, text) %>%
tds_process()
training_corpus = tds_process(lemmatize = "french")
training_corpus = tds_process(lemmatization = "french")
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus = tds_process(lemmatization = "french")
training_corpus = tds_process(training_corpus, lemmatization = "french")
training_corpus
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus_processed = tds_process(training_corpus, lemmatization = "french")
training_corpus = tds_process(segment_size = 200)
training_corpus = tds_process(training_corpus, segment_size = 200)
training_corpus
complete_text_classification %>% filter(date %in% select_date$date[16:20]) %>% sample_n(500) %>% arrange(date, document) %>% select(-work, -date, -page, -block) %>% write_tsv("apply_classification.tsv")
library(tidysupervise)
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
library(readr)
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus
training_corpus_processed = tds_process(training_corpus)
training_corpus_processed = tds_process(training_corpus)
training_corpus_processed
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus %>%
unnest_tokens(token, text) %>%
tds_process()
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus_processed = tds_process(training_corpus, lemmatization = "french")
training_corpus_processed
training_corpus_processed = tds_process(training_corpus, segment_size = 200)
training_corpus_processed = tds_process(training_corpus, segment_size = 200)
training_corpus_processed
#Here we only keep token that have appeared in at least 4 different documents and, at most the 2000 more frequent token.
training_corpus_processed = tds_process(training_corpus, min_doc_count = 4, max_word_set = 2000)
training_corpus_processed
corpus_model = tds_model(training_corpus_processed)
training_corpus_processed
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus
training_corpus_processed = tds_process(training_corpus, training = TRUE)
training_corpus_processed = tds_process(training_corpus, training = TRUE)
training_corpus_processed
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus %>%
unnest_tokens(token, text) %>%
tds_process(training = TRUE)
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus_processed = tds_process(training_corpus, training = TRUE, lemmatization = "french")
training_corpus_processed = tds_process(training_corpus, training = TRUE, lemmatization = "french")
training_corpus_processed
training_corpus_processed = tds_process(training_corpus, training = TRUE, segment_size = 200)
training_corpus_processed
complete_text_classification = readr::read_tsv("corpus_classification.tsv.zip")
select_date = complete_text_classification %>% group_by(date) %>% summarise() %>% sample_n(20)
complete_text_classification %>% group_by(label) %>% summarise(count_label = n())
complete_text_classification %>% group_by(label) %>% summarise(count_label = n()) %>% filter(count_label > 70)
complete_text_classification %>% group_by(label) %>% summarise(count_label = n()) %>% filter(count_label > 70) %>% View()
complete_text_classification %>% group_by(label) %>% summarise(count_label = n()) %>% filter(count_label > 70) %>% filter(label != "guerre" & label != "loisir et transport")
complete_text_classification %>% group_by(label) %>% summarise(count_label = n()) %>% filter(count_label > 70) %>% filter(label != "guerre" & label != "loisir et transport" & label != "publicité")
select_label = complete_text_classification %>% group_by(label) %>% summarise(count_label = n()) %>% filter(count_label > 70) %>% filter(label != "guerre" & label != "loisir et transport" & label != "publicité")
select_label = select_label %>% select(label)
complete_text_classification %>% filter(date %in% select_date$date[1:15]) %>% filter(label %in% select_label$label) %>% sample_n(1500) %>% arrange(date, document) %>% select(-work, -date, -page, -block) %>% write_tsv("sample_classification.tsv")
complete_text_classification %>% filter(date %in% select_date$date[16:20]) %>% filter(label %in% select_label$label) %>% sample_n(1500) %>% arrange(date, document) %>% select(-work, -date, -page, -block) %>% write_tsv("apply_classification.tsv")
complete_text_classification %>% filter(date %in% select_date$date[16:20]) %>% filter(label %in% select_label$label) %>% sample_n(500) %>% arrange(date, document) %>% select(-work, -date, -page, -block) %>% write_tsv("apply_classification.tsv")
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus
training_corpus_processed = tds_process(training_corpus, training = TRUE)
training_corpus_processed = tds_process(training_corpus, training = TRUE)
training_corpus_processed
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus %>%
unnest_tokens(token, text) %>%
tds_process(training = TRUE)
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus_processed = tds_process(training_corpus, training = TRUE, lemmatization = "french")
training_corpus_processed = tds_process(training_corpus, training = TRUE, lemmatization = "french")
training_corpus_processed
training_corpus_processed = tds_process(training_corpus, training = TRUE, segment_size = 200)
training_corpus_processed
library(tidysupervise)
library(readr)
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus
training_corpus %>% group_by(label) %>% summarise()
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus
training_corpus_processed = tds_process(training_corpus, training = TRUE)
training_corpus_processed = tds_process(training_corpus, training = TRUE)
training_corpus_processed
training_corpus = read_tsv(system.file("extdata", "sample_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
training_corpus %>%
unnest_tokens(token, text) %>%
tds_process(training = TRUE)
training_corpus = unnest_tokens(training_corpus, token, text)
training_corpus_processed = tds_process(training_corpus, training = TRUE, lemmatization = "french")
training_corpus_processed = tds_process(training_corpus, training = TRUE, lemmatization = "french")
training_corpus_processed
training_corpus_processed = tds_process(training_corpus, training = TRUE, segment_size = 200)
training_corpus_processed = tds_process(training_corpus, training = TRUE, segment_size = 200)
training_corpus_processed
#Here we only keep token that have appeared in at least 4 different documents and, at most the 2000 more frequent token.
training_corpus_processed = tds_process(training_corpus, training = TRUE, min_doc_count = 4, max_word_set = 2000)
training_corpus_processed
training_corpus_processed = tds_process(training_corpus, training = TRUE)
corpus_model = tds_model(training_corpus_processed)
corpus_model
tds_retrieve(corpus_model)
tds_open(corpus_model)
#Here we set prop_train to 85%, which leaves 15% of the original corpus for testing.
test_model = tds_process(training_corpus, training = TRUE) %>%
tds_model(prop_train = 85)
#Here we set prop_train to 85%, which leaves 15% of the original corpus for testing.
test_model = tds_process(training_corpus, training = TRUE) %>%
tds_test(prop_train = 85)
test_model
test_model %>% filter(result = 0) %>% group_by(original, prediction) %>% summarise(count_misclassification = n()) %>% arrange(-count_misclassification)
test_model %>% filter(result == 0) %>% group_by(original, prediction) %>% summarise(count_misclassification = n()) %>% arrange(-count_misclassification)
test_model %>%
filter(result == 0) %>% #We only keep the "wrong" results
group_by(original, prediction) %>%
summarise(count_misclassification = n()) %>% #We group and summarise all the possible combinations.
arrange(-count_misclassification)
apply_corpus = read_tsv(system.file("extdata", "apply_classification.tsv.zip", package = "tidysupervise"), col_types = "ccc")
apply_corpus_processed = apply_corpus %>%
unnest_tokens(token, text) %>%
tds_process(max_word_set = 0)
apply_corpus_processed
result_model = tds_apply(apply_corpus_processed, model = corpus_model)
result_model
library(tidysupervise)
library(tidysupervise)
tds_generate(dir = "test_corpora")
library(tidysupervise)
tds_generate(dir = "test_corpora")
#' A function to generate a csv file for training out of a directory of text
#'
#' @param dir A directory containing the training corpora, with one document per text file.
#' @param training_file The name of the file containing the output a the function: a dataset linking document name (and, possibly, texts) with empty labels.
#' @param metadata_only Whether the output will contain only the metadata (the name of the documents) or also the text. Set to false by default.
#' @param sample_document A sample size smaller than the number of the document in the directory. When set at 0 (default), sampling is not used.
#' @param output_format The format of the file where the data frame will be stored. Either "tsv" or "csv": "tsv" is usually preferable for textual data, since the tab key is much less used than punctuation.
#' @return A data frame prepared for classification labelling.
#' @examples
#'  tds_generate(dir = "corpora", training_file = "my_corpora.csv", metadata_only = TRUE, sample_document = 500, output_format = "csv")
tds_generate <- function(dir, training_file = "", metadata_only = FALSE, sample_document = 0, output_format = "tsv", max_char = 50000) {
#output_format has to be either csv or tsv.
if (!output_format %in% c("tsv", "csv")) {
output_format = "tsv"
}
#If a training file has not been stated, we will reuse the name of the directory.
if (training_file == "") {
training_file = paste0(dir, ".", output_format)
}
#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Sample the list of files, if sample_document is higher than 0 and smaller than the number of files.
if (sample_document > 0 & length(text_files) > sample_document) {
text_files = sample(text_files, sample_document)
}
#Retrieve all the texts and initialize the classification dataset.
if (metadata_only == FALSE) {
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readChar, nchar = max_char))
} else {
text_data = data_frame(document = basename(text_files),
label = "")
}
if (output_format == "tsv") {
write_tsv(text_data, training_file)
}
if (output_format == "csv") {
write_csv(text_data, training_file)
}
}
tds_generate(dir = "test_corpora")
metadata_only = FALSE
training_file = ""
dir = "test_corpora"
output_format = "tsv"
max_char = 50000
max_char = 500000
#output_format has to be either csv or tsv.
if (!output_format %in% c("tsv", "csv")) {
output_format = "tsv"
}
#If a training file has not been stated, we will reuse the name of the directory.
if (training_file == "") {
training_file = paste0(dir, ".", output_format)
}
#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Sample the list of files, if sample_document is higher than 0 and smaller than the number of files.
if (sample_document > 0 & length(text_files) > sample_document) {
text_files = sample(text_files, sample_document)
}
sample_document = 0
#Sample the list of files, if sample_document is higher than 0 and smaller than the number of files.
if (sample_document > 0 & length(text_files) > sample_document) {
text_files = sample(text_files, sample_document)
}
#Retrieve all the texts and initialize the classification dataset.
if (metadata_only == FALSE) {
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readChar, nchar = max_char))
} else {
text_data = data_frame(document = basename(text_files),
label = "")
}
text_data
#' A function to generate a csv file for training out of a directory of text
#'
#' @param dir A directory containing the training corpora, with one document per text file.
#' @param training_file The name of the file containing the output a the function: a dataset linking document name (and, possibly, texts) with empty labels.
#' @param metadata_only Whether the output will contain only the metadata (the name of the documents) or also the text. Set to false by default.
#' @param sample_document A sample size smaller than the number of the document in the directory. When set at 0 (default), sampling is not used.
#' @param output_format The format of the file where the data frame will be stored. Either "tsv" or "csv": "tsv" is usually preferable for textual data, since the tab key is much less used than punctuation.
#' @return A data frame prepared for classification labelling.
#' @examples
#'  tds_generate(dir = "corpora", training_file = "my_corpora.csv", metadata_only = TRUE, sample_document = 500, output_format = "csv")
tds_generate <- function(dir, training_file = "", metadata_only = FALSE, sample_document = 0, output_format = "tsv", max_char = 500000) {
#output_format has to be either csv or tsv.
if (!output_format %in% c("tsv", "csv")) {
output_format = "tsv"
}
#If a training file has not been stated, we will reuse the name of the directory.
if (training_file == "") {
training_file = paste0(dir, ".", output_format)
}
#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Sample the list of files, if sample_document is higher than 0 and smaller than the number of files.
if (sample_document > 0 & length(text_files) > sample_document) {
text_files = sample(text_files, sample_document)
}
#Retrieve all the texts and initialize the classification dataset.
if (metadata_only == FALSE) {
text_data = data_frame(document = basename(text_files),
label = "",
text = map(text_files, readChar, nchar = max_char))
text_data = text_data %>% unnest(text)
} else {
text_data = data_frame(document = basename(text_files),
label = "")
}
if (output_format == "tsv") {
write_tsv(text_data, training_file)
}
if (output_format == "csv") {
write_csv(text_data, training_file)
}
}
tds_generate(dir = "test_corpora")
test = read_tsv("test_corpora.tsv")
test
tds_generate(dir = "test_corpora", metadata_only = TRUE)
tds_generate(dir = "test_corpora", sample_document = 10)
test = read_tsv("test_corpora.tsv")
test
tds_generate(dir = "test_corpora", metadata_only = TRUE)
training_corpus = tds_associate(dir = "test_corpora", training_metadata = "test_corpora.csv")
#' A function to combine the text with a labelled set of metadata.
#'
#' @param dir A directory containing the training corpora, with one document per text file.
#' @param training_metadata The name of the file containing the name of the name of the documents, the labels and possibly other metadata.
#' @param csv_type A type of csv file: "comma" (by default) uses read_csv while "semicolon" uses read_csv2.
#' @return A data frame combining the texts located in a directory and the labelled data in a tsv/csv file.
#' @examples
#'  tds_associate(dir = "corpora", training_metadata = "my_corpora.csv")
tds_associate <- function(dir, training_metadata = "", csv_type = "comma") {
#Open the training_metadata file with either tsv or csv
if (grepl("tsv$", training_metadata)) {
training_metadata = read_tsv(training_metadata)
}
if (grepl("csv$", training_metadata)) {
if(csv_type == "comma") {
training_metadata = read_csv(training_metadata)
}
if(csv_type == "semicolon"){
training_metadata = read_csv2(training_metadata)
}
}
#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Create the text data frame.
text_data = data_frame(document = basename(text_files),
text = map(text_files, readChar, nchar = max_char))
#Join the metadata on the text data frame.
text_data = text_data %>%
inner_join(training_metadata, by=c("document" = "document"))
return(text_data)
}
training_corpus = tds_associate(dir = "test_corpora", training_metadata = "test_corpora.csv", csv_type = "semicolon")
training_corpus
#' A function to combine the text with a labelled set of metadata.
#'
#' @param dir A directory containing the training corpora, with one document per text file.
#' @param training_metadata The name of the file containing the name of the name of the documents, the labels and possibly other metadata.
#' @param csv_type A type of csv file: "comma" (by default) uses read_csv while "semicolon" uses read_csv2.
#' @return A data frame combining the texts located in a directory and the labelled data in a tsv/csv file.
#' @examples
#'  tds_associate(dir = "corpora", training_metadata = "my_corpora.csv")
tds_associate <- function(dir, training_metadata = "", csv_type = "comma") {
#Open the training_metadata file with either tsv or csv
if (grepl("tsv$", training_metadata)) {
training_metadata = read_tsv(training_metadata)
}
if (grepl("csv$", training_metadata)) {
if(csv_type == "comma") {
training_metadata = read_csv(training_metadata)
}
if(csv_type == "semicolon"){
training_metadata = read_csv2(training_metadata)
}
}
#Get all the files from the directory.
text_files = list.files(dir, full.names = TRUE)
#Create the text data frame.
text_data = data_frame(document = basename(text_files),
text = map(text_files, readChar, nchar = max_char))
text_data = text_data %>% unnest(text)
#Join the metadata on the text data frame.
text_data = text_data %>%
inner_join(training_metadata, by=c("document" = "document"))
return(text_data)
}
training_corpus = tds_associate(dir = "test_corpora", training_metadata = "test_corpora.csv", csv_type = "semicolon")
training_corpus
library(tidysupervise)
??unnest
library(roxygen2)
roxygenise()
library(tidysupervise)
library(roxygen2)
roxygenise()
library(tidysupervise)
library(tidysupervise)
?importFrom
??importFrom
library(tidysupervise)
library(roxygen2)
roxygenise()
library(tidysupervise)
